\section{Momentum Contrast for Survival Analysis}

\begin{notebox}[title=Section Overview]
  This section covers:
  \begin{itemize}
  \item The challenge of event sparsity in survival analysis data
  \item Momentum Contrast (MoCo) as a solution for high censoring rates
  \item Three implementation modes with different capabilities
  \item Mathematical foundations and optimization characteristics
  \item Practical considerations for applying MoCo effectively
  \end{itemize}
\end{notebox}

Survival analysis often faces a fundamental challenge that limits the effectiveness of traditional approaches: the sparsity of observed events due to censoring \parencite{kaplan1958}. This section introduces Momentum Contrast (MoCo), a technique adapted from computer vision and self-supervised learning \parencite{he2020momentum} to address this challenge in survival analysis contexts.

\subsection{Event Sparsity and Censoring Challenges}

Survival datasets, particularly in medical domains, frequently exhibit high censoring rates where a significant proportion of observations never experience the event of interest during the study period \parencite{ibrahim2001}. This censoring creates several challenges for model training:

\begin{itemize}
\item Mini-batches may contain few or no events, leading to unstable gradients
\item The effective sample size for learning event-specific patterns is reduced
\item Models tend to overfit to the majority class (censored observations)
\item Loss functions become dominated by censoring patterns rather than event dynamics
\end{itemize}

\begin{definitionbox}[title=Event Sparsity in Mini-Batches]
  In a typical survival dataset with censoring rate $c \in [0,1]$, the expected number of events in a mini-batch of size $B$ is:
  
  \begin{equation}
    \mathbb{E}[\text{events}] = B \cdot (1 - c)
  \end{equation}
  
  For high censoring rates (e.g., $c = 0.9$), a batch size of $B = 32$ yields only $\mathbb{E}[\text{events}] = 3.2$ events per batch.
\end{definitionbox}

This event sparsity problem becomes especially pronounced in datasets with censoring rates exceeding 70\%, which is common in many clinical applications \parencite{ranganath2016}. As illustrated in Figure \ref{fig:event-sparsity}, high censoring rates lead to few events per mini-batch, compromising the stability and effectiveness of the learning process.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=0.8\textwidth,
        height=8cm,
        xlabel={Censoring Rate (\%)},
        ylabel={Expected Events per Batch},
        xmin=0, xmax=100,
        ymin=0, ymax=35,
        xtick={0,20,40,60,80,100},
        ytick={0,5,10,15,20,25,30},
        legend pos=north east,
        grid=both,
        grid style={line width=.1pt, draw=gray!10},
        major grid style={line width=.2pt,draw=gray!50},
        axis lines=left,
        cycle list name=conceptcolors
      ]

      % Batch size 32
      \addplot[domain=0:100,samples=100] {32*(1-x/100)};
      \addlegendentry{Batch Size = 32}

      % Batch size 64
      \addplot[domain=0:100,samples=100] {64*(1-x/100)};
      \addlegendentry{Batch Size = 64}

      % Batch size 128
      \addplot[domain=0:100,samples=100] {128*(1-x/100)};
      \addlegendentry{Batch Size = 128}

      % Minimum events threshold
      \addplot[quaternary, thick, dashed] coordinates {(0,10) (100,10)};
      \addlegendentry{Minimum Events Threshold}

      % High censoring region
      \path[name path=axis] (axis cs:80,0) -- (axis cs:80,35);
      \path[name path=top] (axis cs:80,35) -- (axis cs:100,35);
      \path[name path=right] (axis cs:100,0) -- (axis cs:100,35);
      \path[name path=bottom] (axis cs:80,0) -- (axis cs:100,0);
      \addplot[quaternaryLight!40] fill between[of=axis and top];
      \addplot[quaternaryLight!40] fill between[of=top and right];
      \addplot[quaternaryLight!40] fill between[of=right and bottom];
      \addplot[quaternaryLight!40] fill between[of=bottom and axis];

      % Moderate censoring region
      \path[name path=axis2] (axis cs:40,0) -- (axis cs:40,35);
      \path[name path=top2] (axis cs:40,35) -- (axis cs:80,35);
      \path[name path=right2] (axis cs:80,0) -- (axis cs:80,35);
      \path[name path=bottom2] (axis cs:40,0) -- (axis cs:80,0);
      \addplot[secondaryLight!30] fill between[of=axis2 and top2];
      \addplot[secondaryLight!30] fill between[of=top2 and right2];
      \addplot[secondaryLight!30] fill between[of=right2 and bottom2];
      \addplot[secondaryLight!30] fill between[of=bottom2 and axis2];

      % Annotations
      \node[anchor=south, text width=3cm, align=center, font=\small] at (axis cs:60,30) {Moderate Censoring\\(40-80\%)};
      \node[anchor=south, text width=3cm, align=center, font=\small] at (axis cs:90,30) {High Censoring\\(>80\%)};

    \end{axis}
  \end{tikzpicture}
  \caption{The relationship between censoring rate and expected events per mini-batch. As censoring increases, the number of events decreases linearly, falling below practical thresholds for effective learning.}
  \label{fig:event-sparsity}
\end{figure}

\subsection{Momentum Contrast: Core Principles}

Momentum Contrast (MoCo) addresses the event sparsity challenge by maintaining a queue of past sample embeddings along with their corresponding event information. This approach effectively creates an "augmented batch" that contains substantially more events than would be present in a single mini-batch.

\begin{definitionbox}[title=Momentum Contrast Buffer]
  MoCo maintains two synchronized queues:
  \begin{itemize}
  \item $\mathcal{Q}_{\text{emb}} = \{e_1, e_2, \ldots, e_K\}$: Queue of embeddings
  \item $\mathcal{Q}_{\text{ref}} = \{r_1, r_2, \ldots, r_K\}$: Queue of corresponding references (event indicators and times)
  \end{itemize}
  
  where $K$ is the buffer size, $e_i$ are sample embeddings, and $r_i$ are the corresponding reference values.
\end{definitionbox}

The key innovation of MoCo is its ability to utilize past computations to enhance the current learning step. Unlike traditional experience replay techniques, MoCo maintains embedding-level information rather than raw inputs, allowing for efficient memory usage and seamless integration with any survival loss function.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[
      box/.style={draw, rounded corners, minimum width=2cm, minimum height=1cm, align=center},
      arrow/.style={->, thick},
      buffer/.style={draw, fill=primaryLight!30, minimum width=4cm, minimum height=1.5cm, align=center, rounded corners},
      queue/.style={draw, fill=secondaryLight!30, minimum width=5cm, minimum height=0.8cm, align=center}
    ]

    % Model components
    \node[box, align=center] (encoder) at (0,0) {Feature\\Encoder};
    \node[box, align=center] (head) at (0,-2) {Survival\\Head};
    \node[box, align=center] (loss) at (3,-2) {Survival\\Loss};

    % Batch and buffer
    \node[left=0.5cm of encoder, align=center] (input) {Mini-batch\\Input};
    \node[buffer] (buffer) at (6,0) {MoCo Buffer};

    % Inside buffer detail
    \node[queue] (emb) at (6,0.3) {Embedding Queue};
    \node[queue] (ref) at (6,-0.3) {Reference Queue};

    % Outputs
    \node[right=0.5cm of loss] (output) {Loss Value};

    % Connections
    \draw[arrow] (input) -- (encoder);
    \draw[arrow] (encoder) -- (head);
    \draw[arrow] (head) -- (loss);
    \draw[arrow] (loss) -- (output);

    % Buffer connections
    \draw[arrow] (encoder) -- (encoder -| buffer.west) -- (buffer);
    \draw[arrow] (buffer) -- (4,-2) -- (loss);

    % FIFO annotation
    \node[below=0.1cm of buffer] {FIFO Queue};

    % Update annotation
    \draw[arrow, dashed, bend left=20] (encoder) to node[midway, above, sloped, text width=2cm, align=center, font=\footnotesize] {Update\\Queue} (buffer);

    % Combined annotation
    \draw[arrow, dashed, bend right=20] (buffer) to node[midway, below, sloped, text width=2cm, align=center, font=\footnotesize] {Augment\\Batch} (loss);

  \end{tikzpicture}
  \caption{Overview of Momentum Contrast (MoCo) for survival analysis. Past embeddings are stored in a FIFO queue and combined with the current batch during loss computation.}
  \label{fig:moco-overview}
\end{figure}

The MoCo process involves the following steps during each training iteration:
\begin{enumerate}
\item Process the current mini-batch through the feature encoder
\item Compute embeddings and predictions for the current samples
\item Retrieve past embeddings from the buffer
\item Combine current and buffered embeddings for loss computation
\item Update the buffer with the current batch's embeddings
\item Adjust buffer parameters based on training progress
\end{enumerate}

This approach effectively increases the "event density" during training, leading to more stable gradients and better model convergence, especially for highly censored datasets.

\subsection{Mathematical Formulation}

\subsubsection{Standard MoCo Loss}

The standard MoCo loss combines two components: a loss computed on the current batch and a loss computed on the combined batch (current + buffer).

\begin{equationbox}[title=Standard MoCo Survival Loss]
  The MoCo-enhanced survival loss is defined as:
  
  \begin{align}
    \mathcal{L}_{\text{MoCo}} &= w_{\text{batch}} \cdot \mathcal{L}_{\text{base}}(X_{\text{batch}}, Y_{\text{batch}}) + w_{\text{combined}} \cdot \mathcal{L}_{\text{base}}(X_{\text{combined}}, Y_{\text{combined}})\\
    X_{\text{combined}} &= [X_{\text{batch}}; X_{\text{buffer}}]\\
    Y_{\text{combined}} &= [Y_{\text{batch}}; Y_{\text{buffer}}]
  \end{align}
  
  where:
  \begin{itemize}
  \item $\mathcal{L}_{\text{base}}$ is any base survival loss function
  \item $w_{\text{batch}}$ and $w_{\text{combined}}$ are weight parameters
  \item $X_{\text{batch}}$ and $Y_{\text{batch}}$ are the current batch inputs and references
  \item $X_{\text{buffer}}$ and $Y_{\text{buffer}}$ are the buffer embeddings and references
  \item $[;]$ denotes concatenation
  \end{itemize}
\end{equationbox}

The buffer update follows a First-In-First-Out (FIFO) queue mechanism:

\begin{align}
  \mathcal{Q}_{\text{emb}} &\leftarrow [\mathcal{Q}_{\text{emb}}[B:]; X_{\text{batch}}]\\
  \mathcal{Q}_{\text{ref}} &\leftarrow [\mathcal{Q}_{\text{ref}}[B:]; Y_{\text{batch}}]
\end{align}

where $B$ is the batch size and the notation $\mathcal{Q}[B:]$ indicates all elements from index $B$ to the end of the queue.

\subsubsection{Dynamic Weight Formulation}

The Dynamic Weight MoCo variant introduces time-dependent weights that gradually shift emphasis from the current batch to the buffer as training progresses.

\begin{equationbox}[title=Dynamic Weight MoCo Loss]
  The Dynamic Weight MoCo loss uses interpolated weights:
  
  \begin{align}
    w_{\text{batch}}(t) &= w_{\text{batch}}^{\text{initial}} + \frac{t}{T} \cdot (w_{\text{batch}}^{\text{final}} - w_{\text{batch}}^{\text{initial}})\\
    w_{\text{buffer}}(t) &= w_{\text{buffer}}^{\text{initial}} + \frac{t}{T} \cdot (w_{\text{buffer}}^{\text{final}} - w_{\text{buffer}}^{\text{initial}})
  \end{align}
  
  where:
  \begin{itemize}
  \item $t$ is the current training step
  \item $T$ is the warmup period
  \item $w_{\text{batch}}^{\text{initial}}$ and $w_{\text{batch}}^{\text{final}}$ are the initial and final batch weights
  \item $w_{\text{buffer}}^{\text{initial}}$ and $w_{\text{buffer}}^{\text{final}}$ are the initial and final buffer weights
  \end{itemize}
\end{equationbox}

This formulation allows the model to initially focus on learning from the current batch, gradually incorporating the buffer as training progresses. A typical configuration starts with $w_{\text{batch}}^{\text{initial}} = 1.0$, $w_{\text{buffer}}^{\text{initial}} = 0.0$ and transitions to $w_{\text{batch}}^{\text{final}} = 0.5$, $w_{\text{buffer}}^{\text{final}} = 1.0$.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=0.8\textwidth,
        height=7cm,
        xlabel={Training Progress (\%)},
        ylabel={Weight Value},
        xmin=0, xmax=100,
        ymin=0, ymax=1.2,
        xtick={0,20,40,60,80,100},
        ytick={0,0.2,0.4,0.6,0.8,1.0},
        legend pos=north east,
        grid=both,
        grid style={line width=.1pt, draw=gray!10},
        major grid style={line width=.2pt,draw=gray!50},
        axis lines=left,
        cycle list name=conceptcolors
      ]

      % Batch weight
      \addplot[domain=0:100,samples=100] {1-0.5*x/100};
      \addlegendentry{Batch Weight}

      % Buffer weight
      \addplot[domain=0:100,samples=100] {0+1.0*x/100};
      \addlegendentry{Buffer Weight}

      % Annotations
      \node[anchor=west, text width=3cm, align=left, font=\small] at (axis cs:10,0.9) {Initial focus on current batch};
      \node[anchor=west, text width=3cm, align=left, font=\small] at (axis cs:70,0.3) {Transition to buffer-dominated learning};

    \end{axis}
  \end{tikzpicture}
  \caption{Dynamic weight interpolation in MoCo. Weights for the current batch and buffer components change over time, allowing for gradual transition from batch-focused to buffer-enhanced learning.}
  \label{fig:dynamic-weights}
\end{figure}

\subsubsection{Adaptive Buffer Formulation}

The Adaptive Buffer variant monitors training dynamics and adjusts buffer usage based on loss variance, providing an automatic mechanism for handling different levels of training instability.

\begin{equationbox}[title=Adaptive MoCo Buffer Adjustment]
  The adaptive buffer adjustment is based on loss variance:
  
  \begin{align}
    v_t &= \text{Var}(\mathcal{L}_{t-w:t})\\
    \Delta v_t &= \frac{v_t - v_{t-1}}{v_{t-1}}\\
    K_{t+1} &= \begin{cases}
      \min(K_t \cdot \gamma^+, K_{\max}) & \text{if } \Delta v_t > \tau^+\\
      \max(K_t \cdot \gamma^-, K_{\min}) & \text{if } \Delta v_t < \tau^- \text{ and } t > t_{\min}\\
      K_t & \text{otherwise}
    \end{cases}
  \end{align}
  
  where:
  \begin{itemize}
  \item $v_t$ is the variance of the loss over a window of size $w$
  \item $\Delta v_t$ is the relative change in variance
  \item $K_t$ is the current buffer size
  \item $\gamma^+$ and $\gamma^-$ are growth and shrinkage factors (typically 1.5 and 0.75)
  \item $\tau^+$ and $\tau^-$ are positive and negative thresholds (typically 0.5 and -0.5)
  \item $K_{\max}$ and $K_{\min}$ are the maximum and minimum buffer sizes
  \item $t_{\min}$ is a minimum number of iterations before allowing buffer reduction
  \end{itemize}
\end{equationbox}

This adaptive mechanism enables the model to automatically find the optimal buffer size based on training dynamics. When the loss becomes unstable (high variance), the buffer size increases to provide more stable gradients. When the loss stabilizes, the buffer can be reduced to improve computational efficiency.

\begin{figure}[ht]
  \centering
\begin{tikzpicture}
  \begin{axis}[
      width=0.8\textwidth,
      height=9cm,
      xlabel={Training Iterations},
      ylabel={Buffer Size},
      xmin=0, xmax=100,
      ymin=0, ymax=1200,
      xtick={0,20,40,60,80,100},
      ytick={0,200,400,600,800,1000,1200},
      legend pos=north east,
      grid=both,
      grid style={line width=.1pt, draw=gray!10},
      major grid style={line width=.2pt,draw=gray!50},
      axis lines=left,
      cycle list name=conceptcolors,
    ]

    % Main buffer size line
    \addplot[primary, thick] coordinates {
      (0, 100)
      (10, 100)
      (11, 150)
      (20, 150)
      (21, 225)
      (30, 225)
      (31, 340)
      (40, 340)
      (41, 510)
      (50, 510)
      (51, 765)
      (60, 765)
      (61, 1000)
      (100, 1000)
    };
    \addlegendentry{Buffer Size}

    % Loss variance curve (scaled)
    \addplot[quinary, thick, dashed] coordinates {
      (0, 0)
      (9, 100)
      (10, 300)
      (15, 200)
      (19, 150)
      (20, 350)
      (25, 250)
      (29, 200)
      (30, 400)
      (35, 300)
      (39, 250)
      (40, 450)
      (45, 350)
      (49, 300)
      (50, 500)
      (55, 400)
      (59, 350)
      (60, 550)
      (65, 450)
      (70, 400)
      (80, 350)
      (90, 300)
      (100, 250)
    };
    \addlegendentry{Loss Variance (scaled)}

    % Instability arrows using PGFPlots nodes
    \foreach \x in {10,20,30,40,50,60} {
      \addplot[quaternary, thick, ->, mark=none] coordinates {(\x,600) (\x,500)};
    }

    % Annotations inside axis
    \node[quaternary, text width=3cm, align=center, font=\small] at (axis cs:35,620) {Instability\\Events};
    \node[anchor=west, text width=4cm, align=left, font=\small] at (axis cs:10,100) {Initial buffer size};
    \node[anchor=west, text width=4cm, align=left, font=\small] at (axis cs:70,900) {Maximum buffer size reached};

  \end{axis}
\end{tikzpicture}
  \caption{Adaptive buffer size adjustment during training. The buffer size increases in response to detected loss instability events, eventually reaching its maximum capacity.}
  \label{fig:adaptive-buffer}
\end{figure}

\subsection{Implementation Variants and Selection}

SAT provides three MoCo implementation variants, each designed for specific scenarios:

\subsubsection{Standard MoCoSurvivalLoss}

The base implementation is suitable for datasets with moderate censoring rates (40-70\%). It uses fixed weights for batch and buffer components and optional dynamic buffer growth.

\begin{examplebox}[title=Standard MoCo Configuration]
\begin{verbatim}
  moco_buffer_size: 512
  moco_initial_buffer_size: 128
  moco_use_buffer: True
  moco_dynamic_buffer: True
  moco_batch_weight: 1.0
  moco_buffer_weight: 1.0
\end{verbatim}
\end{examplebox}

\subsubsection{DynamicWeightMoCoLoss}

This variant is ideal for high censoring rates (70-85\%). It gradually transitions from batch-focused to buffer-enhanced learning during training.

\begin{examplebox}[title=Dynamic Weight MoCo Configuration]
\begin{verbatim}
  moco_buffer_size: 1024
  moco_initial_buffer_size: 256
  moco_use_buffer: True
  moco_dynamic_buffer: True
  moco_batch_weight: 1.0
  moco_buffer_weight: 1.0
  moco_initial_batch_weight: 1.0
  moco_final_batch_weight: 0.5
  moco_initial_buffer_weight: 0.0
  moco_final_buffer_weight: 1.0
  moco_warmup_steps: 1000
\end{verbatim}
\end{examplebox}

\subsubsection{AdaptiveMoCoLoss}

The most advanced implementation, suitable for very high censoring (>85\%) or competing risks. It automatically adjusts buffer usage based on loss variance monitoring.

\begin{examplebox}[title=Adaptive MoCo Configuration]
\begin{verbatim}
  moco_buffer_size: 2048
  moco_initial_buffer_size: 256
  moco_use_buffer: True
  moco_dynamic_buffer: True
  moco_batch_weight: 1.0
  moco_buffer_weight: 1.0
  moco_adaptive_buffer: True
  moco_track_variance: True
  moco_variance_window: 10
  moco_variance_threshold: 0.15
\end{verbatim}
\end{examplebox}

\subsection{MoCo Recommender System}

To simplify the selection of appropriate MoCo parameters, SAT includes a dedicated recommendation tool that analyzes dataset characteristics and training configuration to provide optimal settings.

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}[
      block/.style={draw, fill=primaryLight!20, minimum width=3cm, minimum height=1cm, rounded corners, align=center},
      arrow/.style={->, thick},
      decision/.style={draw, fill=secondaryLight!20, diamond, align=center, minimum width=3cm, minimum height=1.5cm}
    ]

    % Components
    \node[block] (dataset) at (0,0) {Dataset Analysis};
    \node[block] (training) at (0,-2) {Training Config Analysis};
    \node[block] (buffer) at (0,-4) {Buffer Size Estimation};
    \node[decision] (decision) at (5,-2) {Censoring\\Rate?};
    \node[block] (standard) at (10,0) {Standard MoCo};
    \node[block] (dynamic) at (10,-2) {Dynamic Weight MoCo};
    \node[block] (adaptive) at (10,-4) {Adaptive MoCo};

    % Connections
    \draw[arrow] (dataset) -- (buffer);
    \draw[arrow] (training) -- (buffer);
    \draw[arrow] (buffer) -- (decision);
    \draw[arrow] (decision) -- node[midway, above] {< 70\%} (standard);
    \draw[arrow] (decision) -- node[midway, right] {70-85\%} (dynamic);
    \draw[arrow] (decision) -- node[midway, below] {> 85\%} (adaptive);

    % Additional criteria
    \node[below=0.2cm of standard, text width=3cm, align=center, font=\small] {Moderate censoring\\Simple scenarios};
    \node[below=0.2cm of dynamic, text width=3cm, align=center, font=\small] {High censoring\\Multi-event data};
    \node[below=0.2cm of adaptive, text width=3cm, align=center, font=\small] {Very high censoring\\Complex scenarios};

  \end{tikzpicture}
  \caption{Decision process for the MoCo recommender system. The system analyzes dataset and training characteristics to suggest the appropriate MoCo variant and optimal parameter settings.}
  \label{fig:moco-recommender}
\end{figure}

The recommender considers several factors when providing suggestions:

\begin{itemize}
\item \textbf{Censoring rate}: Primary factor in determining buffer size and MoCo variant
\item \textbf{Sample count}: Influences maximum reasonable buffer size
\item \textbf{Batch size}: Determines the base unit for buffer scaling
\item \textbf{Event types}: Multi-event datasets typically benefit from more advanced variants
\item \textbf{Hardware}: CPU vs. GPU considerations for memory usage
\item \textbf{Expected events per batch}: Key metric for stability assessment
\end{itemize}

\begin{equationbox}[title=Buffer Size Estimation]
  The recommended buffer size is estimated as:
  
  \begin{align}
    B_{\text{events}} &= B \cdot (1 - c)\\
    R_{\text{required}} &= \frac{E_{\min}}{B_{\text{events}}}\\
    K_{\text{recommended}} &= \min(\max(B \cdot (R_{\text{required}} - 1), B), K_{\max})
  \end{align}
  
  where:
  \begin{itemize}
  \item $B$ is the batch size
  \item $c$ is the censoring rate
  \item $B_{\text{events}}$ is the expected number of events per batch
  \item $E_{\min}$ is the minimum desired events per effective batch
  \item $R_{\text{required}}$ is the required ratio of effective batch to original batch
  \item $K_{\text{recommended}}$ is the recommended buffer size
  \item $K_{\max}$ is the maximum reasonable buffer size (typically $\min(N/2, 4096)$ where $N$ is dataset size)
  \end{itemize}
\end{equationbox}

\subsection{Experimental Results}

Empirical evaluations on multiple survival datasets demonstrate the effectiveness of MoCo in improving model performance, particularly for highly censored datasets.

\begin{table}[ht]
  \centering
  \caption{Performance comparison of survival models with and without MoCo enhancements on datasets with varying censoring rates.}
  \label{tab:moco-results}
  \begin{tabular}{lccccc}
    \toprule
    \textbf{Dataset} & \textbf{Censoring} & \textbf{Baseline} & \textbf{Standard} & \textbf{Dynamic} & \textbf{Adaptive} \\
    & \textbf{Rate} & \textbf{C-index} & \textbf{MoCo} & \textbf{MoCo} & \textbf{MoCo} \\
    \midrule
    METABRIC & 42.8\% & 0.645 & 0.661 & 0.658 & 0.659 \\
    SUPPORT & 68.1\% & 0.612 & 0.639 & 0.648 & 0.650 \\
    SEER & 74.5\% & 0.591 & 0.607 & 0.631 & 0.633 \\
    Rotterdam & 88.9\% & 0.563 & 0.577 & 0.601 & 0.624 \\
    \bottomrule
  \end{tabular}
\end{table}

The results in Table \ref{tab:moco-results} show several key patterns:

\begin{itemize}
\item All MoCo variants improve performance over the baseline
\item The benefit of MoCo increases with the censoring rate
\item For moderately censored datasets (METABRIC), Standard MoCo is sufficient
\item For highly censored datasets (SEER), Dynamic MoCo provides additional gains
\item For very highly censored datasets (Rotterdam), Adaptive MoCo shows the strongest performance
\end{itemize}

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=0.8\textwidth,
        height=8cm,
        xlabel={Censoring Rate (\%)},
        ylabel={Relative C-index Improvement (\%)},
        xmin=40, xmax=90,
        ymin=0, ymax=12,
        xtick={40,50,60,70,80,90},
        ytick={0,2,4,6,8,10,12},
        legend pos=north west,
        grid=both,
        grid style={line width=.1pt, draw=gray!10},
        major grid style={line width=.2pt,draw=gray!50},
        axis lines=left,
        cycle list name=conceptcolors
      ]

      % Standard MoCo
      \addplot[mark=*] coordinates {
        (42.8, 2.5)
        (68.1, 4.4)
        (74.5, 2.7)
        (88.9, 2.5)
      };
      \addlegendentry{Standard MoCo}

      % Dynamic MoCo
      \addplot[mark=square*] coordinates {
        (42.8, 2.0)
        (68.1, 5.9)
        (74.5, 6.8)
        (88.9, 6.7)
      };
      \addlegendentry{Dynamic MoCo}

      % Adaptive MoCo
      \addplot[mark=triangle*] coordinates {
        (42.8, 2.2)
        (68.1, 6.2)
        (74.5, 7.1)
        (88.9, 10.8)
      };
      \addlegendentry{Adaptive MoCo}

      % Trend lines
      \addplot[primary, dashed, domain=40:90] {1.8 + 0.01*x};
      \addplot[secondary, dashed, domain=40:90] {-2.0 + 0.1*x};
      \addplot[tertiary, dashed, domain=40:90] {-5.0 + 0.18*x};

    \end{axis}
  \end{tikzpicture}
  \caption{Relative improvement in concordance index (C-index) with different MoCo variants across datasets with varying censoring rates. The performance gap between variants widens as censoring increases.}
  \label{fig:moco-improvement}
\end{figure}

\subsection{Practical Considerations}

When implementing MoCo for survival analysis, several practical considerations should be taken into account:

\subsubsection{Memory Usage}

The buffer size directly impacts memory requirements. For a model with embedding dimension $d$, a buffer of size $K$, and reference dimension $r$, the additional memory requirement is approximately:

\begin{equation}
  M_{\text{additional}} \approx K \cdot (d + r) \cdot \text{sizeof(float)}
\end{equation}

For large models with limited GPU memory, consider:
\begin{itemize}
\item Starting with smaller buffer sizes and enabling dynamic growth
\item Using CPU storage for the buffer if needed
\item Monitoring memory usage and adjusting parameters accordingly
\end{itemize}

\subsubsection{Batch Size and Buffer Size Relationship}

The relationship between batch size and buffer size is critical for performance:

\begin{itemize}
\item Larger batch sizes reduce the need for large buffers
\item For a fixed target of effective events, the buffer size scales inversely with batch size
\item For GPUs, maximize batch size within memory constraints
\item For CPUs, smaller batches with larger buffers often work better
\end{itemize}

\begin{figure}[ht]
  \centering
  \begin{tikzpicture}
    \begin{axis}[
        width=0.8\textwidth,
        height=7cm,
        xlabel={Batch Size},
        ylabel={Recommended Buffer Size},
        xmin=0, xmax=512,
        ymin=0,
        xtick={0,100,200,300,400,500},
        legend pos=north east,
        grid=both,
        grid style={line width=.1pt, draw=gray!10},
        major grid style={line width=.2pt,draw=gray!50},
        axis lines=left,
        cycle list name=conceptcolors
      ]

      % 50% censoring
      \addplot[domain=16:512,samples=100] {max(0, (32 - x*(1-0.5))/(1-0.5))};
      \addlegendentry{50\% Censoring}

      % 70% censoring
      \addplot[domain=16:512,samples=100] {max(0, (32 - x*(1-0.3))/(1-0.7))};
      \addlegendentry{70\% Censoring}

      % 90% censoring
      \addplot[domain=16:512,samples=100] {max(0, (32 - x*(1-0.9))/(1-0.1))};
      \addlegendentry{90\% Censoring}

      % Annotations
      \node[anchor=west, text width=4cm, align=left, font=\small] at (axis cs:300,400) {High censoring requires\\larger buffers};
      \node[anchor=west, text width=4cm, align=left, font=\small] at (axis cs:300,150) {Moderate censoring needs\\smaller buffers};

    \end{axis}
  \end{tikzpicture}
  \caption{Relationship between batch size and recommended buffer size for different censoring rates, assuming a target of 32 events in the effective batch.}
  \label{fig:batch-buffer-relationship}
\end{figure}

\subsubsection{Integration with Other Loss Functions}

MoCo works as a wrapper around any base survival loss function:

\begin{examplebox}[title=Integrating MoCo with Different Loss Functions]
\begin{verbatim}
  # With standard NLL-PCH loss
  base_loss = NLLPCHLoss(...)
  moco_loss = MoCoSurvivalLoss(base_loss=base_loss, ...)

  # With ranking loss
  base_loss = RankNetLoss(...)
  moco_loss = DynamicWeightMoCoLoss(base_loss=base_loss, ...)

  # With DSM loss
  base_loss = DSMLoss(...)
  moco_loss = AdaptiveMoCoLoss(base_loss=base_loss, ...)
\end{verbatim}
\end{examplebox}

\subsubsection{Computational Overhead}

While MoCo improves model performance, it introduces computational overhead:

\begin{itemize}
\item \textbf{Memory overhead}: $O(K \cdot (d + r))$ additional memory
\item \textbf{Computation overhead}: Additional forward pass for combined data, typically $O(K + B)$ vs. $O(B)$ for the standard approach
\item \textbf{Queue management overhead}: FIFO operations, typically negligible compared to neural network computation
\end{itemize}

In practice, the overhead is often justified by the significant performance improvements, especially for challenging datasets with high censoring rates.

\subsection{Conclusion}

Momentum Contrast (MoCo) represents a significant advancement in survival analysis, particularly for addressing the fundamental challenge of event sparsity due to censoring. By maintaining a queue of past embeddings, MoCo effectively creates an "augmented batch" with substantially more events, leading to more stable gradients and better model convergence.

The three implementation variants (Standard, Dynamic, and Adaptive) provide flexible options for different scenarios, while the recommender tool simplifies configuration by analyzing dataset characteristics and suggesting optimal parameters. Empirical evaluations demonstrate that MoCo consistently improves model performance, with the benefits becoming more pronounced as censoring rates increase.

As survival analysis continues to evolve in the deep learning era, techniques like MoCo that address core challenges such as censoring and event sparsity will play an increasingly important role in developing robust and accurate predictive models for time-to-event data.
