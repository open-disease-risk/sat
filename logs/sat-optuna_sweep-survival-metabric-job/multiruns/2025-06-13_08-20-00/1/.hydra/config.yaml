task: optuna_sweep
do_sweep: false
do_data_parse: true
do_data_transform: true
do_tokenize: true
do_pretrain: false
do_finetune: true
run_id: ${oc.env:AZUREML_RUN_ID, ""}
parent_run_id: ${oc.env:AZUREML_ROOT_RUN_ID, "."}
base_dir: .
model_dir: ${parent_run_id}/${run_id}
data_dir: ${base_dir}/data
log_dir: ${base_dir}/logs
output_dir: ${hydra:runtime.output_dir}
work_dir: ${hydra:runtime.cwd}
modelhub: ${data_dir}/model-hub
data:
  parse:
    _target_: sat.data.dataset.parse_metabric.metabric
    source: ${data_source}
    processed_dir: ${modelhub}
    n_bins: 10
    encode: ordinal
    strategy: quantile
    name: ${dataset}
  preprocess_data: false
  perform_split: true
  split_col: split
  duration_col: duration
  event_col: event
  id_col: index
  num_events: 1
  transformed_duration_cols:
  - t
  - f
  splits:
  - train
  - valid
  - test
  load:
    _target_: datasets.load_dataset
    path: csv
    data_files:
    - ${modelhub}/${dataset}/${dataset}.csv
    streaming: false
  label_transform:
    buffer_size: 50000
    cuts: ${label_transform_cuts}
    scheme: ${label_transform_scheme}
    save_dir: ${modelhub}/${dataset}
    event_type: null
data_source: ${base_dir}/data/metabric/metabric_IHC4_clinical_train_test.h5
tokenizers:
  tokenizer_dir: ${modelhub}/${dataset}/tokenizer
  tokenize_column: x
  is_split_into_words: false
  do_padding: true
  padding_args:
    direction: right
    pad_to_multiple_of: 1
    padding: max_length
    length: ${tokenizers.max_seq_length}
  do_truncation: true
  truncation_args:
    direction: left
    max_length: ${tokenizers.max_seq_length}
  pad_token: '[PAD]'
  mask_token: '[MASK]'
  unk_token: '[UNK]'
  cls_token: '[CLS]'
  sep_token: '[SEP]'
  tokenizer:
    _target_: tokenizers.Tokenizer
    model:
      _target_: tokenizers.models.WordLevel
      unk_token: ${tokenizers.unk_token}
  pre_tokenizer:
    _target_: tokenizers.pre_tokenizers.WhitespaceSplit
  trainer:
    _target_: tokenizers.trainers.WordLevelTrainer
  special_tokens:
    unk_token: ${tokenizers.unk_token}
    sep_token: ${tokenizers.sep_token}
    pad_token: ${tokenizers.pad_token}
    cls_token: ${tokenizers.cls_token}
    mask_token: ${tokenizers.mask_token}
  num_proc: 4
  cls_model_type: BERT
  max_seq_length: 10
transformers:
  config:
    _target_: transformers.BertConfig
    hidden_size: ${transformer_hidden_size}
    num_hidden_layers: ${transformer_num_hidden_layers}
    num_attention_heads: ${transformer_num_attention_heads}
    intermediate_size: ${transformer_intermediate_size}
    attention_probs_dropout_prob: ${transformer_attention_probs_dropout_prob}
    hidden_act: gelu
    hidden_dropout_prob: ${transformer_hidden_probs_dropout}
    output_attentions: false
    output_hidden_states: true
    initializer_range: ${transformer_initializer_range}
    return_dict: true
tasks:
  losses:
    survival:
      _target_: sat.loss.SATNLLPCHazardLoss
      importance_sample_weights: ${data.label_transform.save_dir}/imp_sample.csv
      num_events: ${data.num_events}
  metrics:
  - _target_: sat.evaluate.eval_modules.ComputeBrier
    cfg: ${data}
    duration_cuts: ${data.label_transform.save_dir}/duration_cuts.csv
    per_horizon: ${brier_per_horizon}
  - _target_: sat.evaluate.eval_modules.ComputeCIndex
    cfg: ${data}
    duration_cuts: ${data.label_transform.save_dir}/duration_cuts.csv
  eval_metric: eval_ipcw_weighted_avg
  eval_metric_greater_is_better: true
  config:
    _target_: sat.models.heads.MTLConfig
    _convert_: partial
    freeze_transformer: false
    initializer_range: ${mtl_initializer_range}
    initializer: ${mtl_initializer}
    pretrained_params: ${transformers}
    sentence_emb: ${sentence_emb}
    token_emb: ${token_emb}
    hidden_size: ${transformer_hidden_size}
    num_features: ${tokenizers.max_seq_length}
    intermediate_size: ${shared_intermediate_size}
    num_labels: ${shared_num_labels}
    batch_norm: ${shared_batch_norm}
    hidden_dropout_prob: ${shared_hidden_dropout_prob}
    bias: ${shared_bias}
    num_hidden_layers: ${shared_num_hidden_layers}
    select_hidden_layers: ${select_hidden_layers}
    return_dict: true
    task_heads:
    - _target_: sat.models.heads.SurvivalConfig
      _recursive_: false
      initializer_range: ${survival_initializer_range}
      initializer: ${survival_initializer}
      num_features: ${shared_num_labels}
      intermediate_size: ${survival_shared_intermediate_size}
      num_hidden_layers: ${survival_shared_num_hidden_layers}
      indiv_intermediate_size: ${survival_intermediate_size}
      indiv_num_hidden_layers: ${survival_num_hidden_layers}
      num_labels: ${data.label_transform.cuts}
      batch_norm: ${survival_batch_norm}
      hidden_dropout_prob: ${survival_hidden_dropout_prob}
      bias: ${survival_bias}
      loss: ${tasks.losses}
      loss_weight: ${survival_loss_weight}
      num_events: ${data.num_events}
trainer:
  custom: false
  training_arguments:
    _target_: transformers.TrainingArguments
    report_to: tensorboard
    optim: adamw_torch
    overwrite_output_dir: true
    num_train_epochs: 500
    per_device_train_batch_size: 256
    per_device_eval_batch_size: 512
    load_best_model_at_end: true
    greater_is_better: ${tasks.eval_metric_greater_is_better}
    eval_strategy: steps
    eval_steps: 1
    logging_strategy: steps
    logging_steps: 1
    save_strategy: steps
    save_steps: 1
    eval_delay: ${warmup_steps}
    weight_decay: ${weight_decay}
    metric_for_best_model: ${tasks.eval_metric}
    learning_rate: ${learning_rate}
    lr_scheduler_type: cosine
    gradient_accumulation_steps: 2
    warmup_steps: ${warmup_steps}
    save_total_limit: 2
    output_dir: ${modelhub}/${dataset}/${modelname}
    save_safetensors: false
    gradient_checkpointing: false
    dataloader_pin_memory: true
    remove_unused_columns: false
callbacks:
- _target_: transformers.EarlyStoppingCallback
  early_stopping_patience: 10
label_transform_scheme: equidistant
label_transform_cuts: 4
one_calibration_bins: 10
likelihood_loss_coeff: 0.5
event_ranking_loss_coeff: 0.1
event_ranking_loss_sigma: 0.1
event_ranking_loss_margin: 0.05
ranking_loss_coeff: 0.2
ranking_loss_sigma: 0.1
ranking_loss_margin: 0.05
calibration_loss_coeff: 0.2
focal_loss_coeff: 0.1
focal_loss_gamma:
- 2.0
observation_ranking_loss_coeff: 0.3
observation_ranking_loss_sigma: 0.1
listmle_epsilon: 1.0e-10
listmle_temperature: 1.0
survrnc_margin: 0.5
survrnc_temperature: 0.1
survrnc_hard_mining: true
survrnc_mining_ratio: 0.3
soap_margin: 0.1
soap_sigma: 1.0
soap_num_pairs: null
soap_sampling_strategy: importance
soap_adaptive_margin: true
ranknet_sigma: 1.0
ranknet_sampling_ratio: 0.3
ranknet_adaptive_sampling: true
dsm_intermediate_size: 64
dsm_num_hidden_layers: 1
dsm_num_mixtures: 4
dsm_distribution: weibull
dsm_temp: 1000.0
dsm_discount: 1.0
dsm_bias: true
dsm_batch_norm: true
dsm_hidden_dropout_prob: 0.2
dsm_elbo: true
mensa_intermediate_size: 64
mensa_num_hidden_layers: 1
mensa_num_mixtures: 4
mensa_distribution: weibull
mensa_temp: 1000.0
mensa_discount: 1.0
mensa_bias: true
mensa_batch_norm: true
mensa_hidden_dropout_prob: 0.2
mensa_elbo: true
mensa_event_distribution: true
mensa_dependency_regularization: 0.01
loss_balancing_scale_alpha: 0.9
loss_balancing_scale_eps: 1.0e-08
loss_balancing_grad_alpha: 0.9
loss_balancing_grad_eps: 1.0e-08
loss_balancing_uncertainty_sigma: 1.0
loss_balancing_adaptive_alpha: 0.9
loss_balancing_adaptive_eps: 1.0e-08
loss_balancing_adaptive_window_size: 100
loss_balancing_adaptive_adaptation_rate: 0.005
detect_anomalies: true
brier_per_horizon: false
nllpch_per_event: false
l1_per_event: false
mse_per_event: false
ce_per_event: false
learning_rate: 0.0011899542405860882
weight_decay: 0.013870223324141805
token_emb: 5
sentence_emb: 3
mtl_initializer_range: 0.02
mtl_initializer: kaiming_normal
transformer_layer_norm: false
transformer_initializer_range: 0.01
transformer_hidden_size: 16
transformer_intermediate_size: 64
transformer_num_attention_heads: 2
transformer_num_hidden_layers: 4
survival_initializer_range: 0.02
survival_initializer: kaiming_normal
survival_shared_intermediate_size: 4
survival_shared_num_hidden_layers: 0
survival_batch_norm: false
survival_hidden_dropout_prob: false
survival_bias: true
survival_num_hidden_layers: 1
survival_loss_weight: 1.0
classification_initializer_range: 0.02
classification_initializer: xavier_normal
classification_shared_intermediate_size: 4
classification_shared_num_hidden_layers: 0
classification_event_time_thr: 0.5
classification_intermediate_size: 64
classification_batch_norm: false
classification_hidden_dropout_prob: false
classification_bias: true
classification_num_hidden_layers: 0
classification_loss_weight: 1.0
regression_initializer_range: 0.02
regression_initializer: kaiming_normal
regression_shared_intermediate_size: 4
regression_shared_num_hidden_layers: 0
regression_intermediate_size: 32
regression_batch_norm: false
regression_hidden_dropout_prob: false
regression_bias: true
regression_num_hidden_layers: 0
regression_num_labels: 1
regression_loss_weight: 1.0
moco_buffer_size: 1024
moco_use_buffer: true
moco_batch_weight: 1.0
moco_buffer_weight: 1.0
moco_dynamic_buffer: true
moco_initial_buffer_size: 128
moco_initial_batch_weight: 1.0
moco_final_batch_weight: 0.5
moco_initial_buffer_weight: 0.0
moco_final_buffer_weight: 1.0
moco_warmup_steps: 1000
moco_variance_window: 10
moco_variance_threshold: 0.1
moco_min_buffer_ratio: 0.25
moco_max_buffer_ratio: 1.0
warmup_steps: 80
select_hidden_layers: null
dropout: 0.05
transformer_attention_probs_dropout_prob: ${dropout}
transformer_hidden_probs_dropout: ${dropout}
shared_intermediate_size: 16
shared_batch_norm: true
shared_hidden_dropout_prob: 0.05
shared_bias: true
shared_num_hidden_layers: 0
shared_num_labels: 8
dataset: metabric
modelname: survival
survival_intermediate_size: 8
job: job
seed: null
task_name: sat-${task}-${modelname}-${dataset}-${job}
cv:
  k: null
pipeline_use_cv: true
pipeline_use_ci: false
replication: null
multiple_replications: true
cv_kfold: 3
alpha: 0.05
error: 0.1
'n': 10
use_val: false
less_than_n: 10
optuna:
  metric:
  - ipcw.mean
  - brier.mean
  metric_direction:
  - maximize
  - minimize
  pruner:
    _target_: optuna.pruners.MedianPruner
    interval_steps: 5
    n_startup_trials: 5
    n_warmup_steps: 40
  study_overwrite: true
