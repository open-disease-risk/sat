config:
  _target_: sat.models.bert.configuration_bert.NumericBertConfig
  hidden_size: ${transformer_hidden_size}
  num_hidden_layers: ${transformer_num_hidden_layers}
  num_attention_heads: ${transformer_num_attention_heads}
  intermediate_size: ${transformer_intermediate_size}
  attention_probs_dropout_prob: ${transformer_attention_probs_dropout_prob}
  hidden_act: 'gelu'
  hidden_dropout_prob: ${transformer_hidden_probs_dropout}
  output_attentions: False
  output_hidden_states: True
  initializer_range: ${transformer_initializer_range}
  layer_norm: ${transformer_layer_norm}
  return_dict: true
  vocab_size: 1000
